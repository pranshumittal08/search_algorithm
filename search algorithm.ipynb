{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "applied-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "from joblib import Parallel,delayed\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-reaction",
   "metadata": {},
   "source": [
    "## Algorithm steps:\n",
    "- create a spell corrector using the Minimum edit distance DP algorithm\n",
    "- use regular expressions to find the best match given the corrected query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pending-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"Guvi Geek Networks Private Limited\", \"Maximl Labs\", \"Pur Energy Private Limited\", \"Agnikul Cosmos\", \"The ePlane Company\", \n",
    "                 \"PYTORQ Solutions Private Limited\", \"Bigphi Technologies\", \"Ather Energy Private Limited\", \"Rekindle Automations Private Limited\", \n",
    "                 \"Aerostrovilos Energy Private Limited\", \"Impensus Electronics\", \"Doodhbhandaar\", \"Swapeco\", \"Statlogic India Private Limited\",\n",
    "                 \"YNOS Venture Engine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "israeli-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing the company names\n",
    "company_names = [name.split(\" \") for name in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "american-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set([word for name in company_names  for word in name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bound-shoulder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Impensus', 'Statlogic', 'Electronics', 'Aerostrovilos', 'India', 'Engine', 'Solutions', 'The', 'Guvi', 'Company', 'Swapeco', 'Networks', 'Technologies', 'Bigphi', 'Labs', 'Rekindle', 'Energy', 'Private', 'Venture', 'Pur', 'Maximl', 'Cosmos', 'YNOS', 'Agnikul', 'Automations', 'PYTORQ', 'Doodhbhandaar', 'Limited', 'Geek', 'Ather', 'ePlane']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-deposit",
   "metadata": {},
   "source": [
    "The above names will used as learned vocabulary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-original",
   "metadata": {},
   "source": [
    "### Minimum Edit Distance Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "accepting-diabetes",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def min_edit_distance(vocab, query):\n",
    "    '''\n",
    "    Input: \n",
    "        query: a string we are starting with\n",
    "        vocab: list of words containing the strings we want to end with\n",
    "    Output:\n",
    "        med_word: word that is at minimum distance from the query word i.e. most similar to the query word \n",
    "    '''\n",
    "    \n",
    "    all_distances = []\n",
    "    query = query.lower()\n",
    "    ins_cost = 1 #cost of inserting a character from the target word i.e. word in vocab\n",
    "    del_cost = 1 #Cost of deleting a character from the source word i.e. word in query\n",
    "    rep_cost = 2 #Cost of swapping the characters from source and target word\n",
    "    #computing the min-edit-distance for each word in the vocab\n",
    "    for target_word in vocab:\n",
    "        n = len(query) #Length of the query word\n",
    "        m = len(target_word) #Length of the target word\n",
    "        target_word = target_word.lower()\n",
    "        #Creating a matrix/array of size (n+1, m+1)\n",
    "        distance = np.zeros((n+1,m+1), dtype = \"int\")\n",
    "        \"\"\"The below two lines are for updating the distance matrix with cost of insertion if we were to create the target word given NULL and \n",
    "        with the cost of deletion if we were to obtain NULL given the source word\"\"\"\n",
    "        distance[0,:] = range(m+1) #Updating the distance matrix with cost of insertion  \n",
    "        distance[:,0] = range(n+1) #Updating the distance matrix with cost of deletion \n",
    "        #Code to fill remaining of the distance matrix\n",
    "        for i in range(1,n+1):\n",
    "            for j in range(1, m+1):\n",
    "                if query[i-1] != target_word.lower()[j-1]:\n",
    "                    rep_cost = 2\n",
    "                else:\n",
    "                    rep_cost = 0\n",
    "                distance[i][j] = min(distance[i-1,j-1] + rep_cost, distance[i-1,j] + del_cost, distance[i,j-1] + ins_cost)\n",
    "        all_distances.append(distance[-1,-1])\n",
    "        med_index = all_distances.index(min(all_distances)) #index of the word with min distance\n",
    "    med_word = vocab[med_index] #most similar word\n",
    "    \n",
    "    return med_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "waiting-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_match(data, prob_tokens):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        data: List of all the company names in the dataset\n",
    "        prob_tokens: List containing the tokens with minimum distance from the query words\n",
    "    Output:\n",
    "        best_match: Name of the company with that closely matches the search query\n",
    "    \"\"\"\n",
    "    count = collections.defaultdict()\n",
    "\n",
    "    for company in data:\n",
    "        count[company] = 0\n",
    "        for token in prob_tokens:\n",
    "            if token in company:\n",
    "                count[company] += 1\n",
    "    best_match = sorted(count.items(), key = lambda x: x[1], reverse=True)[0][0]\n",
    "    return best_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bibliographic-count",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Search company:  pus\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pur Energy Private Limited\n",
      "0.0010 secs\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Search company: \").split(\" \")\n",
    "start = time.time()\n",
    "prob_tokens = [min_edit_distance(vocab, word) for word in query] #List of most probable tokens\n",
    "print(f\"Best Match: {best_match(data, prob_tokens)}\") #Obtaining the best search result\n",
    "print(f\"{time.time() - start:.4f} secs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-ghost",
   "metadata": {},
   "source": [
    "### Parallelizing loops using concurrent programming (Alternate technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "removed-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(target_word, query):\n",
    "    '''\n",
    "    Input: \n",
    "        query: a string we are starting with\n",
    "        target_word: a string we want to end with\n",
    "    Output:\n",
    "        med_word: word that is at minimum distance from the query word i.e. most similar to the query word \n",
    "    '''\n",
    "    \n",
    "    query = query.lower()\n",
    "    target_word = target_word.lower()\n",
    "    ins_cost = 1 #cost of inserting a character from the target word i.e. word in vocab\n",
    "    del_cost = 1 #Cost of deleting a character from the source word i.e. word in query\n",
    "    rep_cost = 2 #Cost of swapping the characters from source and target word\n",
    "    #computing the min-edit-distance for each word in the vocab\n",
    "    n = len(query) #Length of the query word\n",
    "    m = len(target_word) #Length of the target word\n",
    "    #Creating a matrix/array of size (n+1, m+1)\n",
    "    distance = np.zeros((n+1,m+1), dtype = \"int\")\n",
    "    \"\"\"The below two lines are for updating the distance matrix with cost of insertion if we were to create the target word given NULL and \n",
    "    with the cost of deletion if we were to obtain NULL given the source word\"\"\"\n",
    "    distance[0,:] = range(m+1) #Updating the distance matrix with cost of insertion  \n",
    "    distance[:,0] = range(n+1) #Updating the distance matrix with cost of deletion \n",
    "    #Code to fill remaining of the distance matrix\n",
    "    for i in range(1,n+1):\n",
    "        for j in range(1, m+1):\n",
    "            if query[i-1] != target_word.lower()[j-1]:\n",
    "                rep_cost = 2\n",
    "            else:\n",
    "                rep_cost = 0\n",
    "            distance[i][j] = min(distance[i-1,j-1] + rep_cost, distance[i-1,j] + del_cost, distance[i,j-1] + ins_cost)\n",
    "\n",
    "    return distance[-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "caring-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to compute distances parallely for all the words in the vocab\n",
    "def most_similar_word(vocab, source_word):\n",
    "    all_distances = Parallel(n_jobs = -1, backend = \"threading\")(delayed(get_distance)(target_word, source_word) for target_word in vocab)\n",
    "    med_index = all_distances.index(min(all_distances)) #index of the word with min distance\n",
    "    med_word = vocab[med_index] #most similar word\n",
    "    return med_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "outdoor-mirror",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Search company:  ynot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best match: YNOS Venture Engine\n",
      "0.0130 secs\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Search company: \").split(\" \") #input sequence splitted into tokens\n",
    "\n",
    "start = time.time() #start time\n",
    "prob_tokens = Parallel(n_jobs = -1, backend = \"threading\")(delayed(most_similar_word)(vocab, word) for word in query)\n",
    "\n",
    "print(f\"Best match: {best_match(data, prob_tokens)}\") #Obtaining the best search result\n",
    "print(f\"{time.time() - start:.4f} secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-academy",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-punch",
   "metadata": {},
   "source": [
    "Even though concurrent programming was used, no performance improvement was witnessed instead there was increase in computation time. This is why we stick to the vanilla min edit distance algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-success",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crest",
   "language": "python",
   "name": "crest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
